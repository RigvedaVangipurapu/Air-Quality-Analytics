{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/13 20:09:43 WARN Utils: Your hostname, Rigvedas-MacBook-Air-7.local resolves to a loopback address: 127.0.0.1; using 192.168.1.249 instead (on interface en0)\n",
      "25/02/13 20:09:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/02/13 20:09:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/13 20:09:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Write to PostgreSQL\") \\\n",
    "    .config(\"spark.jars\", \"/Users/rigvedavangipurapu/Documents/AirQualityProject/Spark_Utils/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"X-API-Key\": \"9b7c23f6701f7f8e923a5691c6b67d1361bd044b308a8f863502d1190cbe7435\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_data_for_city(city_data: tuple) -> List[Tuple[str, Dict]]:\n",
    "    \"\"\"\n",
    "    Helper function to process a single city. This runs on executor nodes.\n",
    "    Returns list of (city, sensor_dict) tuples.\n",
    "    \"\"\"\n",
    "    city, (lat, lon) = city_data\n",
    "    base_url = \"https://api.openaq.org/v3/locations\"\n",
    "    params = {\n",
    "        'coordinates': f\"{lat},{lon}\",\n",
    "        'radius': 15000,\n",
    "        'limit': 15\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        sensors = []\n",
    "        for location in data.get('results', []):\n",
    "            for sensor in location.get('sensors', []):\n",
    "                sensors.append({\n",
    "                    'sensor_id': sensor['id'],\n",
    "                    'parameter': sensor['parameter']['name'],\n",
    "                    'units': sensor['parameter']['units'],\n",
    "                    'location_name': location['name']\n",
    "                })\n",
    "        # Return list of (city, sensor) tuples\n",
    "        return [(city, sensor) for sensor in sensors]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {city}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_ids(coordinates_dict: Dict):\n",
    "\n",
    "    # Convert coordinates dictionary to list of tuples for parallelization\n",
    "    cities_data = list(coordinates_dict.items())\n",
    "    \n",
    "    # Create RDD from cities data and collect sensor information\n",
    "    cities_rdd = spark.sparkContext.parallelize(cities_data)\n",
    "    sensor_data_rdd = cities_rdd.flatMap(get_sensor_data_for_city)\n",
    "    \n",
    "    return sensor_data_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_measurements_for_sensors(sensor_ids_rdd: 'RDD[str]', date_to: str, date_from: str) -> 'RDD[Tuple[str, Dict]]':\n",
    "    \"\"\"\n",
    "    Returns RDD of (sensor_id, measurement_dict) tuples.\n",
    "    \"\"\"\n",
    "    def fetch_sensor_measurements(sensor_id: str) -> List[Tuple[str, Dict]]:\n",
    "        url = f\"https://api.openaq.org/v3/sensors/{sensor_id}/measurements/daily\"\n",
    "        params = {'datetime_to': date_to, 'datetime_from': date_from}\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            measurements = response.json()\n",
    "            return [(sensor_id, measurement) for measurement in measurements.get('results', [])]\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {sensor_id}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    return sensor_ids_rdd.flatMap(fetch_sensor_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyze_city_data(cities, date_to, date_from):\n",
    "    \"\"\"\n",
    "    Main function to analyze air quality data for all cities.\n",
    "    \"\"\"\n",
    "    # Get sensor IDs for all cities\n",
    "    print(\"Fetching sensor information...\")\n",
    "    sensor_data_rdd = get_sensor_ids(cities)  # RDD[(city, sensor_dict)]\n",
    "# Display the dataset\n",
    "    # print('Sensor Data RDD :',sensor_data_rdd.show(5))\n",
    "\n",
    "    # Create (sensor_id, city) pairs\n",
    "    sensor_city_rdd = sensor_data_rdd.flatMap(\n",
    "        lambda x: [(x[1]['sensor_id'], x[0])]  # (sensor_id, city)\n",
    "    )\n",
    "# Display the dataset\n",
    "    # print('Sensor City RDD :',sensor_city_rdd.show(5))\n",
    "\n",
    "\n",
    "    sensor_ids_rdd = sensor_city_rdd.keys()  # RDD[sensor_id]\n",
    "# Display the dataset\n",
    "    # print('Sensor IDs RDD :',sensor_ids_rdd.show())\n",
    "\n",
    "\n",
    "    # Use correct function with date parameters\n",
    "    measurements_rdd = fetch_measurements_for_sensors(sensor_ids_rdd, date_to, date_from)\n",
    "# Display the dataset\n",
    "    # print('Measurements RDD :',measurements_rdd.show())\n",
    "\n",
    "\n",
    "    joined_rdd = sensor_city_rdd.join(measurements_rdd)  # RDD[(sensor_id, (city, measurement_dict))]\n",
    "    \n",
    "    # Convert to Row objects\n",
    "    def to_row(sensor_data: Tuple[str, Tuple[str, Dict]]) -> Row:\n",
    "        sensor_id, (city, measurement) = sensor_data\n",
    "        return Row(\n",
    "            city=city,\n",
    "            location=measurement.get('location', 'Unknown'),\n",
    "            parameter=measurement.get('parameter', {}).get('name', 'Unknown'),\n",
    "            units=measurement.get('parameter', {}).get('units', 'Unknown'),\n",
    "            date=measurement.get('period', {}).get('datetimeTo', {}).get('utc', ''),\n",
    "            value=measurement.get('value'),\n",
    "            sensor_id=sensor_id\n",
    "        )\n",
    "    \n",
    "# Apply transformation to the dataset\n",
    "    final_rdd = joined_rdd.map(to_row)\n",
    "    \n",
    "    return spark.createDataFrame(final_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sensor information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+-----+--------------------+-----+---------+\n",
      "|       city|location|parameter|units|                date|value|sensor_id|\n",
      "+-----------+--------+---------+-----+--------------------+-----+---------+\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-02T08:00:00Z|0.453|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-03T08:00:00Z|0.507|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-04T08:00:00Z|0.548|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-05T08:00:00Z|0.617|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-06T08:00:00Z|0.476|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-07T08:00:00Z|0.503|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-08T08:00:00Z|0.391|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-09T08:00:00Z|0.496|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-10T08:00:00Z|0.375|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-11T08:00:00Z|0.492|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-12T08:00:00Z|0.472|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-13T08:00:00Z|0.377|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-14T08:00:00Z| 0.44|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-15T08:00:00Z|0.505|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-16T08:00:00Z|0.606|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-17T08:00:00Z|0.436|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-18T08:00:00Z|0.325|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-19T08:00:00Z|0.415|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-20T08:00:00Z|0.482|    25472|\n",
      "|Los Angeles| Unknown|       co|  ppm|2020-01-21T08:00:00Z| 0.58|    25472|\n",
      "+-----------+--------+---------+-----+--------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sensor_id=25472),\n",
       " Row(sensor_id=25473),\n",
       " Row(sensor_id=25474),\n",
       " Row(sensor_id=25192),\n",
       " Row(sensor_id=25193),\n",
       " Row(sensor_id=25194),\n",
       " Row(sensor_id=23019),\n",
       " Row(sensor_id=25195),\n",
       " Row(sensor_id=25196)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_df = analyze_city_data(\n",
    "    {\"Los Angeles\": (34.0522, -118.2437)},\n",
    "    date_to='2020-12-31',\n",
    "    date_from='2020-01-01'\n",
    ")\n",
    "# Display the dataset\n",
    "la_df.show()\n",
    "la_df.select('sensor_id').distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sensor information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error fetching 7971870: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/7971870/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 7762999: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/7762999/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 7979412: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/7979412/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 7971901: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/7971901/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 8520935: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/8520935/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 8520936: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/8520936/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 8520925: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/8520925/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 8520920: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/8520920/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 8520938: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/8520938/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "Error fetching 8520933: 429 Client Error: Too Many Requests for url: https://api.openaq.org/v3/sensors/8520933/measurements/daily?datetime_to=2020-12-31&datetime_from=2020-01-01\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-----+--------------------+------+---------+\n",
      "|    city|location|parameter|units|                date| value|sensor_id|\n",
      "+--------+--------+---------+-----+--------------------+------+---------+\n",
      "|New York| Unknown|       co|  ppm|2020-01-02T05:00:00Z| 0.205|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-03T05:00:00Z|   0.7|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-04T05:00:00Z| 0.668|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-05T05:00:00Z| 0.483|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-06T05:00:00Z| 0.114|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-07T05:00:00Z| 0.457|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-08T05:00:00Z| 0.417|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-09T05:00:00Z|   0.1|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-10T05:00:00Z| 0.386|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-11T05:00:00Z|  0.69|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-12T05:00:00Z| 0.382|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-13T05:00:00Z| 0.114|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-14T05:00:00Z| 0.376|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-15T05:00:00Z| 0.442|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-16T05:00:00Z| 0.348|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-17T05:00:00Z| 0.174|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-18T05:00:00Z| 0.143|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-19T05:00:00Z| 0.568|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-20T05:00:00Z| 0.236|     2016|\n",
      "|New York| Unknown|       co|  ppm|2020-01-21T05:00:00Z|0.0619|     2016|\n",
      "+--------+--------+---------+-----+--------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ny_df = analyze_city_data(\n",
    "    {\"New York\": (40.7128, -74.0060)},\n",
    "    date_to='2020-12-31',\n",
    "    \n",
    "    date_from='2020-01-01'\n",
    ")\n",
    "# Display the dataset\n",
    "ny_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sensor_id=2016),\n",
       " Row(sensor_id=673),\n",
       " Row(sensor_id=2018),\n",
       " Row(sensor_id=2644),\n",
       " Row(sensor_id=2645),\n",
       " Row(sensor_id=2646),\n",
       " Row(sensor_id=1143),\n",
       " Row(sensor_id=1128),\n",
       " Row(sensor_id=1145),\n",
       " Row(sensor_id=1098),\n",
       " Row(sensor_id=1099),\n",
       " Row(sensor_id=23341),\n",
       " Row(sensor_id=1103),\n",
       " Row(sensor_id=671)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny_df.select('sensor_id').distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(location='Unknown')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny_df.select('location').distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "url = \"jdbc:postgresql://localhost:5432/openaq\"\n",
    "\n",
    "ny_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/openaq\") \\\n",
    "    .option(\"dbtable\", \"dummy_city\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", <password>) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "439ffbdb69c01f9de1260e4a84aa607bef665b2ea2ded18dcfc14cc5ddfca346"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
